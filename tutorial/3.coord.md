# Fault Tolerance

If you completed parts
[one](https://github.com/SyncFree/crdtdb/blob/master/tutorial/1-get-started.md)
and
[two](https://github.com/SyncFree/crdtdb/blob/master/tutorial/2.vnode.md)
you have a very simple distributed database. However, if you lose a
node, then the data on that node is no-longer available. Usually we
want to store multiple copies of our CRDTs. Extra copies provide fault
tolerance, and lower tail latencies. So far our application hashes the
key to a partition and stores the value there. How do we decide where
to put replicas? And how many?

## N Val

In riak and riak-core the `N-Val` of a datum is how many replicas of
that value will be stored. By default this value is three. Knowing
that we're going to store N replicas is the first step, the preference
list is how we decide where.

## The Preference List

Typically in a riak_core application we use the `Preference List` to
decide where replicas should go. Figure 1 is a conceptual
ring.

[img1]: ./ring-n3.jpeg "Figure 1: Replicating to N partitions"
![Figure 1: Replicating to N Partitions][img1]

As discussed in part two, the hashspace is divided into ranges and a
key hashes to a range, or partition. The preference list for a key are
the next N-1 partitions on the ring. In this case, if a key
`<<"artist>>,<<"REM">>` hashes to partition at index `k`, then the
partitions at index `k+1` and `k+2` also store a replica of the
data. The Preference List is the list of partitions (and the nodes
that host) them, for a given hashed Key and N-Val.

In tutorial part two we asked riak-core to give us a preference list
of length `1` for our hashed key. If that node was down, we'd be
unable to read or write our data.

### Primary and Fallback Preference Lists

What if the nodes that are responsible for any of those N partitions
are offline?

Those N nodes in Figure 1 are the `Primary Prefence List`. They are
the home of the hashed key. However, if any of the nodes that make up
that prefernce list are down, we can store to fallbacks. We decide on
fallbacks by continuing to walk around the ring to the next available
partitions. A partition that handles a request, but is not on the
Primary Preference List, is called a `Fallback` or `Secondary`.

### Strict and Sloppy Quora

As well as determining _where_ our replicas are stored, the Primary
and Secondary Preference Lists can be used for tuning the consistency
of our application. For example, if our application would like to
favour a greater degree of consistency over availabilty, we can demand
that we only ever use the Primary Prefence List when doing reads and
writes, and even that a majority of that Preferce List make up the
Quorum of nodes that particpate in a request. This way any read and
write pair must hit an overlapping set of partitions, and we get a
more consistent view of our data.

At the other extreme, if being available to write, at all costs,
matters more, we can create a Prefernece List that is composed of
_any_ available nodes, and only requires a single one of them to
acknowledge a write.

### R and W Values

We call the partitions that reply to a read request the Read Quorum,
and from now on, I'll talk about the `R` value of a read request to
denote the number of partitions we want to respond before we consider
the request a success.

We call the partitions that reply to a write request the Write Quorum,
and henceforth I shall refer to the `W` value when we talk about the
number of partitions we want to acknowledge a write before it is
considered successful.

## Enter The Coordinator

In riak-core applications, we typically write a module to act as a
`Coordinator` for each request type. In a Key/Value store, like
`CRDTDB`, it makes sense for us to write a coordinator for Writes, and
a coordinator for Reads. The coordinator's role will be to generate a
preference list, and coordinate calling the vnodes and tallying their
responses. Usually a coordinator is implemented as a `Finite State
Machine`.

### Erlang FSMs

So far we've only talked about Erlang the language and runtime, but
usually when Erlang is discussed what is really meant is
`Erlang/OTP`. OTP is a set of libraries, patterns, and `behaviours`
that make writing real-world fault tolerant applications easier. One
of these behaviours is called `gen_fsm`. Hopefully as CS academics you
already know what a `Finite State Machine` is. The Erlang/OTP
behaviour `gen_fsm` specifies a set of callback functions, if we
implement these functions, Erlang/OTP will manage our FSM for
us. `gen_fsm` makes writing an FSM process in Erlang simple.

[Here](http://learnyousomeerlang.com/finite-state-machines) is a great
tutorial resource on gen_fsm, if you want to read that before
proceeding, I won't stop you. Or you can try digest
[this drier alternative](http://www.erlang.org/documentation/doc-4.8.2/doc/design_principles/fsm.html).

## CRDTDB N,R,W Values

In a production application you may wish to allow the user to specify
the N, R, and W values. Riak offers even more tunables (PR, PW, DW,
nofound\_ok, basic\_quorum, sloppy\_quorum, etc) which you can read
about (at length)
[here](http://basho.com/understanding-riaks-configurable-behaviors-part-1/). There
are a lot of subtle options and trade-offs. For those who want a quick
overview there is a brief, highlevel description for Riak's tunables
[here](http://docs.basho.com/riak/latest/dev/advanced/cap-controls/).

For `CRDTDB` we'll start off with a hardcoded `N` of `3` and `R` and
`W` values of `2`.

I'm not going to put _all_ the code in this document, have a look at
the `crdtdb_put_fsm.erl` and `crdtdb_get_fsm.erl` for the full src.

### The Write Coordinator

The write coordinator's job is to:

1. Compute a Preference List
2. Call `crdtdb_vnode:put/3` for the preference list
3. Await `W` response
4. Return a result to the client

Each of these steps could map to a `State` in the FSM, with the
`Events` of our FSM moving the coordinator through the states. But I
think it makes sense to have just two states: `execute` to generate
the preflist and call the vnodes, and `await_responses` to gather
replies.

With `gen_fsm` modules we add a callback named for each `state` with
the first argument of that callback being the `event` that has
triggered the call. `Events` are just messages received by the FSM
process. I'm not going into to much detail about the scaffolding
provided by riak-core for communication between coordinators and
vnodes. Maybe another time.

Have a look at the file
[crdt_put_fsm.erl](https://github.com/SyncFree/crdtdb/blob/master/src/crdt_put_fsm.erl)
for the implementation. Clearly not enterprise grade stuff, but enough
for now.

### The Read Coordinator

The read coordinators job is to:

1. Compute a Preference List
2. Call `crdtdb_vnode:get/2` with the PrefList
3. Await `R` responses
4. _Merge_ the received CRDTs to a single value
5. Return a result to the client

In this case the obvious states are as before `execute` and
`await_responses`. We can merge whenever we received a response. In
the case we receive a responses we cannot merge for type differences,
we just error (for now.)

Have a look at the code in
[crdt_get_fsm.erl](https://github.com/SyncFree/crdtdb/blob/master/src/crdt_get_fsm.erl)
for the full implementation.

### Update The Client

Now that we have coordinators, the client doesn't need to calculate
the preflist. It does however need to spawn a process per request, and
wait for a response from the request. The code changes are in the repo
if you want to see the difference.

## Test Drive

Now we can `make devrel` for our new code, and join our cluster
together as in part one.

We can run some requests as in part two. The difference is, we can
kill nodes, in the cluster, and still handle requests. Though we're no
finished yet.

## About Those Fallbacks

We have one job left to do. When a fallback partition handles a write
request it stores the data. But the data doesn't really _belong_
there. It would be great if our cluster was self-healing. Maybe the
fallback stored a write because the primary was down, or maybe the
primary was just unreachable (network partition?), whichever,
riak-core has a way to get the data to its proper home. The process is
called `Hinted Handoff` and I'll cover it tutorial part four.





